#!/usr/bin/env python3
from pathlib import Path
import os
import sys

# Load .env
from dotenv import load_dotenv
env_path = Path(__file__).resolve().parent.parent / ".env"
load_dotenv(dotenv_path=env_path)

import weaviate
from weaviate import AuthApiKey

# LangChain imports
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Weaviate

# OpenAIâ€based embedding
from embedding import get_embedding_function

# â”€â”€â”€ CONFIGURATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
WEAVIATE_URL     = os.getenv("WEAVIATE_URL")
WEAVIATE_API_KEY = os.getenv("WEAVIATE_API_KEY")
CLASS_NAME       = "DC_POC"  # Match your chatbot's index_name
DATA_DIR         = Path(__file__).parent / "data" / "documents"

# Validate environment variables
if not WEAVIATE_URL or not WEAVIATE_API_KEY:
    raise ValueError("Missing WEAVIATE_URL or WEAVIATE_API_KEY in .env file")

print(f"ğŸ”— Connecting to Weaviate at {WEAVIATE_URL}")

# â”€â”€â”€ CONNECT TO WEAVIATE CLOUD (v3 client) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    client = weaviate.Client(
        url=WEAVIATE_URL,
        auth_client_secret=AuthApiKey(api_key=WEAVIATE_API_KEY),
        additional_headers={
            "X-OpenAI-Api-Key": os.getenv("OPENAI_API_KEY", "")
        }
    )
    
    # Test connection
    if client.is_ready():
        print("âœ… Successfully connected to Weaviate")
    else:
        raise ConnectionError("Weaviate client is not ready")
        
except Exception as e:
    print(f"âŒ Failed to connect to Weaviate: {e}")
    raise

# â”€â”€â”€ RESET SCHEMA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    if client.schema.exists(CLASS_NAME):
        print(f"ğŸ”„ Deleting existing class `{CLASS_NAME}`")
        client.schema.delete_class(CLASS_NAME)
    
    print(f"â• Creating class `{CLASS_NAME}`")
    client.schema.create_class({
        "class": CLASS_NAME,
        "vectorizer": "none",  # We provide our own vectors
        "properties": [
            {
                "name": "content",
                "dataType": ["text"],
                "description": "The text content of the document chunk"
            },
            {
                "name": "source",
                "dataType": ["text"],  # Changed from "string" to "text"
                "description": "Source PDF filename"
            }
        ]
    })
    print(f"âœ… Class `{CLASS_NAME}` created successfully")
    
except Exception as e:
    print(f"âŒ Schema operation failed: {e}")
    raise

# â”€â”€â”€ SETTING EMBEDDING FUNCTIONâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("ğŸ”§ Setting up embedding function and vector store...")
try:
    EMBED_FN = get_embedding_function()
    print("âœ… Embedding function initialized")
except Exception as e:
    print(f"âŒ Failed to initialize embedding function: {e}")
    raise

# â”€â”€â”€ TEST EMBEDDING FUNCTION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def test_embedding_function():
    """Quick test to verify embeddings are working"""
    print("\nğŸ§ª Testing embedding function...")
    try:
        test_text = "The Dark Energy Survey studies cosmic acceleration"
        embedding = EMBED_FN.embed_query(test_text)
        print(f"   âœ… Embedding generated: {len(embedding)} dimensions")
        print(f"   ğŸ“Š Sample values: [{embedding[0]:.4f}, {embedding[1]:.4f}, ...]")
        return True
    except Exception as e:
        print(f"   âŒ Embedding test failed: {e}")
        return False

# Call the test
if test_embedding_function():
    print("   ğŸ‰ Embedding function is working correctly\n")
else:
    print("   âš ï¸  Warning: Embedding function may have issues")
    exit(1)

# â”€â”€â”€ VECTOR STORE SETUP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    store = Weaviate(
        client=client,
        index_name=CLASS_NAME,
        text_key="content",
        attributes=["source"],
        embedding=EMBED_FN,
        by_text=False,
    )
    print("âœ… Vector store initialized")
except Exception as e:
    print(f"âŒ Failed to initialize vector store: {e}")
    raise

# â”€â”€â”€ INGEST PDFS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1200,
    chunk_overlap=200,
    length_function=len,
    separators=["\n\n", "\n", ". ", " ", ""]
)

# Check if data directory exists
if not DATA_DIR.exists():
    print(f"âŒ Data directory not found: {DATA_DIR}")
    print(f"   Please create it and add PDF files")
    exit(1)

pdfs = sorted(DATA_DIR.glob("*.pdf"))
if not pdfs:
    print(f"âš ï¸  No PDF files found in {DATA_DIR}")
    exit(1)

print(f"\nğŸ“‚ Found {len(pdfs)} PDF(s) in {DATA_DIR}")
print("="*60)

total_chunks = 0
for idx, pdf in enumerate(pdfs, start=1):
    try:
        print(f"\n[{idx}/{len(pdfs)}] Processing: {pdf.name}")
        
        # Load PDF
        docs = PyPDFLoader(str(pdf)).load()
        print(f"   ğŸ“„ Loaded {len(docs)} page(s)")
        
        # Split into chunks
        chunks = splitter.split_documents(docs)
        print(f"   âœ‚ï¸  Split into {len(chunks)} chunk(s)")
        
        # Add metadata
        for chunk in chunks:
            chunk.metadata["source"] = pdf.name
        
        # Add to vector store
        store.add_documents(chunks)
        total_chunks += len(chunks)
        print(f"   âœ… Added to vector store")
        
    except Exception as e:
        print(f"   âŒ Error processing {pdf.name}: {e}")
        continue

print("\n" + "="*60)
print(f"âœ… Ingestion complete: {total_chunks} total chunks from {len(pdfs)} PDFs")

# â”€â”€â”€ CHUNK QUALITY ANALYSIS [COMMENT IT OUT LATER]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nğŸ“Š Chunk Quality Analysis:")
print("="*60)

sample_chunks = store.similarity_search("DES project overview", k=3)
for i, chunk in enumerate(sample_chunks, 1):
    content = chunk.page_content
    print(f"\nChunk {i}:")
    print(f"  Length: {len(content)} characters (~{len(content.split())} words)")
    print(f"  Source: {chunk.metadata.get('source', 'unknown')}")
    print(f"  Preview: {content[:150]}...")
    
    # Check if chunks are complete thoughts
    if not content.strip().endswith(('.', '!', '?', '\n')):
        print(f"  âš ï¸  Warning: Chunk may be cut mid-sentence")

# â”€â”€â”€ SIMPLE QUERY TEST â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nğŸ” Testing similarity searchâ€¦")
print("="*60)

test_query = "How many documents do you currently have access to?"
print(f"Query: '{test_query}'\n")

try:
    hits = store.similarity_search_with_score(test_query, k=4)
    
    if not hits:
        print("âš ï¸  No results found")
    else:
        print(f"ğŸ“Š Top {len(hits)} matches:\n")
        for i, (doc, score) in enumerate(hits, start=1):
            snippet = doc.page_content.replace("\n", " ").strip()[:200]
            src = doc.metadata.get("source", "unknown")
            print(f"{i}. Score: {score:.4f}")
            print(f"   Source: {src}")
            print(f"   Content: {snippet}...")
            print()
            
except Exception as e:
    print(f"âŒ Search failed: {e}")

print("="*60)
print("ğŸ‰ Script completed successfully!")
